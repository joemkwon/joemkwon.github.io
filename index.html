<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="google-site-verification" content="5PIFWdNCbrEHXrkpVpL_Aq-HkoxYokrFJEffGZVGEsM" />
    <meta name="description" content="Joe Kwon works on AI safety and governance, focusing on risks from internal deployment, automated R&D, and concentration of power. Background in cognitive science and moral cognition at MIT and Yale.">

    <!-- Open Graph / Social Sharing -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://joe-kwon.com/">
    <meta property="og:title" content="Joe Kwon">
    <meta property="og:description" content="Joe Kwon works on AI safety and governance, focusing on risks from internal deployment, automated R&D, and concentration of power.">
    <meta property="og:image" content="https://joe-kwon.com/profile.png">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@joemkwon">
    <meta name="twitter:title" content="Joe Kwon">
    <meta name="twitter:description" content="Joe Kwon works on AI safety and governance, focusing on risks from internal deployment, automated R&D, and concentration of power.">
    <meta name="twitter:image" content="https://joe-kwon.com/profile.png">

    <!-- Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Person",
        "name": "Joe Kwon",
        "url": "https://joe-kwon.com",
        "image": "https://joe-kwon.com/profile.png",
        "jobTitle": "AI Safety Researcher",
        "description": "Works on AI safety and governance, focusing on risks from internal deployment, automated R&D, and concentration of power.",
        "alumniOf": [
            {
                "@type": "CollegeOrUniversity",
                "name": "MIT"
            },
            {
                "@type": "CollegeOrUniversity",
                "name": "Yale University"
            }
        ],
        "sameAs": [
            "https://x.com/joemkwon",
            "https://www.linkedin.com/in/joe-k-5ba472119/",
            "https://unfancy.substack.com/"
        ]
    }
    </script>

    <title>Joe Kwon</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,400;0,6..72,500;1,6..72,400&family=Syne:wght@400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <div class="grain"></div>
    <div class="cursor-glow"></div>

    <nav class="top-nav">
        <a href="#about">About</a>
        <a href="#work">Work</a>
        <a href="#journey">Journey</a>
        <a href="#misc">Misc</a>
        <a href="#contact">Contact</a>
    </nav>

    <canvas id="flow-field"></canvas>

    <main>
        <section class="hero">
            <div class="hero-glow"></div>
            <div class="hero-portrait">
                <img src="profile.png" alt="Joe Kwon">
            </div>
            <h1><span class="gradient-text">Joe Kwon</span></h1>
            <p class="tagline">Trying to help steer towards better (AI-entangled) futures!</p>
        </section>

        <section id="about" class="about">
            <div class="bio-controls">
                <button class="bio-toggle active" data-level="brief">brief</button>
                <button class="bio-toggle" data-level="more">more</button>
                <button class="bio-toggle" data-level="full">full story</button>
            </div>

            <div class="bio-content" data-level="brief">
                <p>
                    AI is poised to be deeply transformative. I think about how to make that go well!
                </p>
            </div>

            <div class="bio-content" data-level="more" style="display: none;">
                <p>
                    I think AI is likely to be deeply transformative—for better or worse—and I work on AI safety and governance to help things go well. Right now that means thinking about risks from internal deployment, automated R&D, and concentration of power. Before this I spent time in cognitive science, studying moral cognition and how humans infer mental states from sparse evidence.
                </p>
                <p>
                    The through-line is curiosity about minds, artificial and otherwise. I like questions that sit at the boundary between understanding how things work and deciding how they should.
                </p>
            </div>

            <div class="bio-content" data-level="full" style="display: none;">
                <p>
                    I think AI is likely to be one of the most transformative developments in human history. My work focuses on what happens as these systems get more capable and more embedded in decisions that matter. Some of that is technical: how do we steer these systems, catch failures before they compound, understand what's actually going on inside them. Some of it is governance: how we track the pace of automated R&D, what happens when capability concentrates faster than our ability to course-correct.
                </p>
                <p>
                    I started my research journey in cognitive science, studying how humans make sense of each other: how we infer mental states from physical traces, how we reconstruct events we never witnessed, how we figure out what someone values from what they do and don't do. I also worked on moral cognition—building neuro-symbolic computational models of moral judgment and drawing on frameworks like contractualism, universalizability, and virtual bargaining to understand how people interpret moral rules, decide when exceptions are warranted, and negotiate competing moral demands. More recently I've been on the AI side directly: red-teaming language models, probing what they know and how they represent it, building benchmarks and evals, testing methods for steering their behavior from the inside.
                </p>
                <p>
                    The through-line is curiosity about minds, artificial and otherwise. I find myself drawn to questions that sit at the boundary between understanding cognition and shaping it, between describing how things work and deciding how they should. I care about rigor but I'm suspicious of fields that mistake formalism for understanding. I think the hard problems are often the ones that resist clean framing, and I'd rather sit with something messy and real than solve something precise and beside the point.
                </p>
            </div>
        </section>

        <section id="work" class="work section-divider">
            <div class="work-header">
                <h2>work</h2>
                <div class="work-controls">
                    <button class="work-toggle active" data-view="recent">recent</button>
                    <button class="work-toggle" data-view="all">all</button>
                </div>
            </div>

            <ul class="project-list" data-view="recent">
                <li>
                    <a href="#">
                        <span class="project-name">Evaluating Contextual Illegality: AI Compliance in Corporate Law Scenarios</span>
                        <span class="project-desc">Testing how AI systems handle context-dependent legal compliance</span>
                        <span class="project-meta">Aka, Kwon, & Kolt · under review</span>
                    </a>
                </li>
                <li>
                    <a href="#">
                        <span class="project-name">Measuring AI R&D Automation</span>
                        <span class="project-desc">Assessing the pace and extent of AI-driven automation in research and development</span>
                        <span class="project-meta">Kwon, Padarath, Chan, Greaves, & Anderljung · GovAI, forthcoming</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2601.08005">
                        <span class="project-name">Internal Deployment Gaps in AI Regulation</span>
                        <span class="project-desc">How current AI governance misses risks from internal use of AI systems</span>
                        <span class="project-meta">Kwon & Casper · under review</span>
                    </a>
                </li>
                <li>
                    <a href="https://osf.io/preprints/psyarxiv/wfj4q_v1">
                        <span class="project-name">What Do Moral Rules Mean?</span>
                        <span class="project-desc">How people interpret and generalize moral principles</span>
                        <span class="project-meta">Kwon, Wong, & Levine · Society for Philosophy and Psychology, 2025</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2505.09662">
                        <span class="project-name">Large Language Models Are More Persuasive Than Incentivized Human Persuaders</span>
                        <span class="project-desc">Comparing AI and human persuasion in real-time interactive settings</span>
                        <span class="project-meta">Schoenegger et al. (40 authors), 2025</span>
                    </a>
                </li>
            </ul>

            <ul class="project-list" data-view="all" style="display: none;">
                <li>
                    <a href="#">
                        <span class="project-name">Evaluating Contextual Illegality: AI Compliance in Corporate Law Scenarios</span>
                        <span class="project-desc">Testing how AI systems handle context-dependent legal compliance</span>
                        <span class="project-meta">Aka, Kwon, & Kolt · under review</span>
                    </a>
                </li>
                <li>
                    <a href="#">
                        <span class="project-name">Measuring AI R&D Automation</span>
                        <span class="project-desc">Assessing the pace and extent of AI-driven automation in research and development</span>
                        <span class="project-meta">Kwon, Padarath, Chan, Greaves, & Anderljung · GovAI, forthcoming</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2601.08005">
                        <span class="project-name">Internal Deployment Gaps in AI Regulation</span>
                        <span class="project-desc">How current AI governance misses risks from internal use of AI systems</span>
                        <span class="project-meta">Kwon & Casper · under review</span>
                    </a>
                </li>
                <li>
                    <a href="https://osf.io/preprints/psyarxiv/wfj4q_v1">
                        <span class="project-name">What Do Moral Rules Mean?</span>
                        <span class="project-desc">How people interpret and generalize moral principles</span>
                        <span class="project-meta">Kwon, Wong, & Levine · Society for Philosophy and Psychology, 2025</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2505.09662">
                        <span class="project-name">Large Language Models Are More Persuasive Than Incentivized Human Persuaders</span>
                        <span class="project-desc">Comparing AI and human persuasion in real-time interactive settings</span>
                        <span class="project-meta">Schoenegger et al. · 2025</span>
                    </a>
                </li>
                <li>
                    <a href="https://www.centeraipolicy.org/work/ai-agents-governing-autonomy-in-the-digital-age">
                        <span class="project-name">AI Agents: Governing Autonomy in the Digital Age</span>
                        <span class="project-desc">Policy proposals for autonomous AI systems and their risks</span>
                        <span class="project-meta">Kwon · Center for AI Policy, 2025</span>
                    </a>
                </li>
                <li>
                    <a href="https://www.centeraipolicy.org/work/ai-at-the-cyber-frontier-securing-americas-digital-future">
                        <span class="project-name">AI at the Cyber Frontier: Securing America's Digital Future</span>
                        <span class="project-desc">Cybersecurity implications of evolving AI capabilities, with policy guidance for Congress</span>
                        <span class="project-meta">Williams & Kwon · Center for AI Policy, 2025</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2411.07213">
                        <span class="project-name">Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks</span>
                        <span class="project-desc">Evaluating methods for steering language model behavior</span>
                        <span class="project-meta">Brumley, Kwon, Krueger, Krasheninnikov, & Anwar · NeurIPS MINT Workshop, 2024</span>
                    </a>
                </li>
                <li>
                    <a href="https://escholarship.org/uc/item/1t2568mw">
                        <span class="project-name">Neuro-Symbolic Models of Human Moral Judgment</span>
                        <span class="project-desc">Computational models of how humans make moral judgments</span>
                        <span class="project-meta">Kwon, Levine, & Tenenbaum · CogSci, 2024</span>
                    </a>
                </li>
                <li>
                    <a href="https://openreview.net/forum?id=uSvN2oozRK">
                        <span class="project-name">Does It Know?: Probing for Uncertainty in Language Model Latent Beliefs</span>
                        <span class="project-desc">Investigating what language models represent about their own uncertainty</span>
                        <span class="project-meta">Huang & Kwon · NeurIPS Workshop, 2023</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2306.09442">
                        <span class="project-name">Explore, Establish, Exploit: Red Teaming Language Models from Scratch</span>
                        <span class="project-desc">An automated pipeline for discovering language model vulnerabilities</span>
                        <span class="project-meta">Casper, Lin, Kwon, Culp, & Hadfield-Menell · 2023</span>
                    </a>
                </li>
                <li>
                    <a href="https://escholarship.org/uc/item/3w59797h">
                        <span class="project-name">When It's Not Out of Line to Get Out of Line</span>
                        <span class="project-desc">Principles of universalizability, welfare, and harm in moral cognition</span>
                        <span class="project-meta">Kwon, Tan, Tenenbaum, & Levine · CogSci, 2023</span>
                    </a>
                </li>
                <li>
                    <a href="https://neurips.cc/virtual/2022/58821">
                        <span class="project-name">Values Shape Optimizers Shape Values</span>
                        <span class="project-desc">On the feedback loop between human values and AI optimization</span>
                        <span class="project-meta">Kwon · NeurIPS Human-Centered AI Workshop, 2022</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2206.15474">
                        <span class="project-name">Forecasting Future World Events With Neural Networks</span>
                        <span class="project-desc">Building ML systems to predict real-world events</span>
                        <span class="project-meta">Zou, Xiao, Jia, Kwon, et al. · NeurIPS, 2022</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/1911.11132">
                        <span class="project-name">Scaling Out-of-Distribution Detection for Real-World Settings</span>
                        <span class="project-desc">Detecting when ML models encounter unfamiliar inputs</span>
                        <span class="project-meta">Hendrycks, Basart, Mazeika, Zou, Kwon, et al. · ICML, 2022</span>
                    </a>
                </li>
                <li>
                    <a href="https://escholarship.org/uc/item/46r7b6ft">
                        <span class="project-name">Flexibility in Moral Cognition: When Is It Okay to Break the Rules?</span>
                        <span class="project-desc">How humans reason about exceptions to moral principles</span>
                        <span class="project-meta">Kwon, Tenenbaum, & Levine · CogSci, 2022</span>
                    </a>
                </li>
                <li>
                    <a href="https://pubmed.ncbi.nlm.nih.gov/35901411/">
                        <span class="project-name">Social Inferences from Physical Evidence via Bayesian Event Reconstruction</span>
                        <span class="project-desc">How we infer social information from physical traces</span>
                        <span class="project-meta">Lopez-Brau, Kwon, & Jara-Ettinger · Journal of Experimental Psychology: General, 2022</span>
                    </a>
                </li>
                <li>
                    <a href="https://escholarship.org/uc/item/43n9v4z9">
                        <span class="project-name">Detecting the Involvement of Agents Through Physical Reasoning</span>
                        <span class="project-desc">Inferring whether an agent was present from physical evidence</span>
                        <span class="project-meta">Lopez-Brau, Kwon, McBean, Yildirim, & Jara-Ettinger · CogSci, 2021</span>
                    </a>
                </li>
                <li>
                    <a href="https://cogsci.mindmodeling.org/2020/papers/0085/">
                        <span class="project-name">Mental State Inference from Indirect Evidence Through Bayesian Event Reconstruction</span>
                        <span class="project-desc">Computational models of inferring mental states from physical traces</span>
                        <span class="project-meta">Lopez-Brau, Kwon, & Jara-Ettinger · CogSci, 2020</span>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/1902.00163">
                        <span class="project-name">Lift-the-Flap: What, Where and When for Context Reasoning</span>
                        <span class="project-desc">Object recognition using recurrent attentional models</span>
                        <span class="project-meta">Zhang, Tseng, Montejo, Kwon, & Kreiman · 2019</span>
                    </a>
                </li>
            </ul>
        </section>

        <section id="journey" class="journey section-divider">
            <h2>the path here</h2>

            <div class="timeline">
                <article class="timeline-entry">
                    <span class="timeline-era">pre-college</span>
                    <p>
                        Mostly spent my time hanging out with friends, consuming a ton of online content, and not really having any direction in life. Mildly nihilistic, honestly. But experiencing CTY and Canada/USA Mathcamp was special and invigorating—the first environments where I felt genuinely excited about ideas and the people around me.
                    </p>
                </article>

                <article class="timeline-entry">
                    <span class="timeline-era">Yale</span>
                    <p>
                        Studied CS and psychology. The summer before sophomore year, I worked with Gabriel Kreiman and Mengmi Zhang at the Harvard/MIT Center for Brains, Minds, and Machines on visual cognition and context reasoning. It was my first research experience and I'm grateful they invested their time in a mostly floundering freshman. That led me to Julian Jara-Ettinger's lab, where I spent the rest of undergrad building computational models of how people infer social and mental information from physical evidence—reconstructing events we never witnessed, figuring out what someone was thinking from what they left behind. Julian taught me how to think carefully and precisely about cognitive modeling, and I owe him a lot.
                    </p>
                </article>

                <article class="timeline-entry">
                    <span class="timeline-era">early AI safety</span>
                    <p>
                        Around 2020 I started paying attention to how much emergent capability was showing up in AI systems. Worked on one of OpenAI's early RLHF projects under Long Ouyang and Jeff Wu—my first hands-on experience with LLMs, and it got me scaling pilled. Then at Berkeley with Jacob Steinhardt and Dan Hendrycks, I worked on out-of-distribution detection, forecasting, and building evaluations for ML systems. I'm grateful to both groups for teaching me what careful empirical ML research looks like and for taking a chance on me early.
                    </p>
                </article>

                <article class="timeline-entry">
                    <span class="timeline-era">MIT</span>
                    <p>
                        Joined Josh Tenenbaum's Computational Cognitive Science Lab, working closely with Sydney Levine on moral and social cognition—how people reason about rules, norms, and each other. We built neuro-symbolic models that tried to capture the structure of moral judgment, which I think matters for AI alignment too. Separately, worked with Stephen Casper and Dylan Hadfield-Menell on red-teaming methods to systematically find where language models fail. I learned a tremendous amount from everyone in both groups—Josh and Sydney especially shaped how I think about cognition and ethics, and Stephen showed me what relentless and creative research looks like.
                    </p>
                </article>

                <article class="timeline-entry">
                    <span class="timeline-era">LG AI Research</span>
                    <p>
                        In 2023 I spent about a year as a research engineer on multi-lingual LLMs under Honglak Lee, working with Lajanugen Logeswaran, Dongsub Shim, and Tolga Ergen. Synthetic data, pretraining, finetuning, evals. One thread I liked: leveraging language-invariant concepts so models can learn new languages more efficiently.
                    </p>
                </article>

                <article class="timeline-entry">
                    <span class="timeline-era">steering and probing</span>
                    <p>
                        From late 2023 into 2024, I worked with David Krueger's group testing activation steering methods. At the time it was genuinely unclear how well these techniques actually worked, what exactly you could do with them, and where they broke down—we wanted to figure that out.
                    </p>
                </article>

                <article class="timeline-entry">
                    <span class="timeline-era">policy</span>
                    <p>
                        In 2025 I moved to policy work—first at the Center for AI Policy, writing reports on AI agents, cybersecurity, and autonomous systems, then GovAI's DC fellowship, working on risks from internal AI deployment and metrics for tracking automated AI R&D. This was refreshing because the questions felt immediately important and impactful. I enjoyed communicating ideas and recommendations to people—tens of thousands read my reports in total—and it led to being invited as a panelist on a Georgetown × World Bank conference on "Making AI Work: What Firms and Workers Need."
                    </p>
                </article>

                <article class="timeline-entry">
                    <span class="timeline-era">now</span>
                    <p>
                        Astra Fellow working with Tom Davidson and Fabien Roger on <a href="https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power#32-secret-ai-loyalties">secretly loyal AI</a>—the risk that an AI system could be deliberately trained to appear aligned with an institution's goals while covertly serving a different actor's interests. I'm focused on threat modeling and designing ML experiments that stress-test this scenario and would be useful for the broader research agenda.
                    </p>
                </article>
            </div>
        </section>

        <section id="misc" class="misc section-divider">
            <h2>rabbit holes</h2>

            <details class="misc-category">
                <summary>reading</summary>
                <ul class="misc-list compact">
                    <li>The Gentle Romance: Stories of AI and humanity — Richard Ngo</li>
                    <li>The Night Circus — Erin Morgenstern</li>
                    <li>The Book of Five Rings — Miyamoto Musashi</li>
                </ul>
            </details>

            <details class="misc-category">
                <summary>listening</summary>
                <div class="listening-grid">
                    <div class="genre-group">
                        <span class="genre-label">hip hop</span>
                        <div class="album-item">
                            <span class="album-name">I LAY DOWN MY LIFE FOR YOU</span>
                            <span class="album-artist">JPEGMAFIA</span>
                            <span class="album-genre">experimental / industrial</span>
                        </div>
                        <div class="album-item">
                            <span class="album-name">LP! (Offline)</span>
                            <span class="album-artist">JPEGMAFIA</span>
                            <span class="album-genre">experimental / glitch</span>
                        </div>
                    </div>
                    <div class="genre-group">
                        <span class="genre-label">jazz(y)</span>
                        <div class="album-item">
                            <span class="album-name">The Black Saint and the Sinner Lady</span>
                            <span class="album-artist">Charles Mingus</span>
                            <span class="album-genre">avant-garde</span>
                        </div>
                        <div class="album-item">
                            <span class="album-name">Hot Rats</span>
                            <span class="album-artist">Frank Zappa</span>
                            <span class="album-genre">jazz-rock</span>
                        </div>
                    </div>
                    <div class="genre-group">
                        <span class="genre-label">art pop</span>
                        <div class="album-item">
                            <span class="album-name">LUX</span>
                            <span class="album-artist">Rosalía</span>
                            <span class="album-genre">orchestral</span>
                        </div>
                        <div class="album-item">
                            <span class="album-name">La Vida Era Más Corta</span>
                            <span class="album-artist">Milo j</span>
                            <span class="album-genre">contemporary folk</span>
                        </div>
                        <div class="album-item">
                            <span class="album-name">Vanisher, Horizon Scraper</span>
                            <span class="album-artist">Quadeca</span>
                            <span class="album-genre">folktronica</span>
                        </div>
                    </div>
                    <div class="genre-group">
                        <span class="genre-label">electronic</span>
                        <div class="album-item">
                            <span class="album-name">I Love My Computer</span>
                            <span class="album-artist">Ninajirachi</span>
                            <span class="album-genre">house / dance / pop</span>
                        </div>
                        <div class="album-item">
                            <span class="album-name">Allbarone</span>
                            <span class="album-artist">Daxter Dury</span>
                            <span class="album-genre">synth pop / electropop</span>
                        </div>
                        <div class="album-item">
                            <span class="album-name">The Provocateur</span>
                            <span class="album-artist">ADÉLA</span>
                            <span class="album-genre">pop / dance / house</span>
                        </div>
                    </div>
                    <div class="genre-group">
                        <span class="genre-label">rock</span>
                        <div class="album-item">
                            <span class="album-name">Fetch</span>
                            <span class="album-artist">Melt-Banana</span>
                            <span class="album-genre">noise / experimental</span>
                        </div>
                        <div class="album-item">
                            <span class="album-name">Pain to Power</span>
                            <span class="album-artist">Maruja</span>
                            <span class="album-genre">post-punk / jazz</span>
                        </div>
                    </div>
                </div>
            </details>

            <details class="misc-category">
                <summary>looking</summary>
                <p class="misc-note">Updating soon.</p>
            </details>

            <details class="misc-category">
                <summary>bookmarks</summary>
                <ul class="misc-list compact">
                    <li class="misc-note-inline">More coming soon.</li>
                    <li><a href="https://www.omarchishti.com/">Omar Chishti</a></li>
                    <li><a href="https://hoyeonchang.github.io/">Hoyeon Chang</a></li>
                </ul>
            </details>
        </section>

        <footer id="contact">
            <p>Say hello</p>
            <nav class="links">
                <a href="mailto:joekwon333@gmail.com">Email</a>
                <a href="https://www.linkedin.com/in/joe-k-5ba472119/">LinkedIn</a>
                <a href="https://x.com/joemkwon">Twitter</a>
                <a href="https://unfancy.substack.com/">Substack</a>
                <a href="https://www.albumoftheyear.org/user/kwon/ratings/highest/">AOTY</a>
            </nav>

            <nav class="bg-controls">
                <button class="bg-toggle active" data-mode="ocean">ocean</button>
                <button class="bg-toggle" data-mode="fractal">fractal</button>
                <button class="bg-toggle" data-mode="flow">flow</button>
                <button class="bg-toggle" data-mode="constellation">constellation</button>
                <button class="bg-toggle" data-mode="lorenz">lorenz</button>
                <button class="bg-toggle" data-mode="voronoi">voronoi</button>
                <button class="bg-toggle" data-mode="lissajous">lissajous</button>
            </nav>

            <p class="colophon">
                My favorite color is the blue you see when you're on a boat and the
                seafloor drops away into nothing, that moment where the water goes from
                turquoise to something almost black and you realize there's a mile of
                water beneath you and it's beautiful and terrifying.
                Gerstner waves and perspective fade. Caustics from light bending through
                surfaces.
            </p>
        </footer>
    </main>

    <script src="script.js"></script>
    <script data-goatcounter="https://joekwon.goatcounter.com/count"
            async src="//gc.zgo.at/count.js"></script>
</body>
</html>
